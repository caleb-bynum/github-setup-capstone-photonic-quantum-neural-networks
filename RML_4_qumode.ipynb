{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b560f09",
   "metadata": {},
   "source": [
    "### Author: Samuel Smith, Arthur Lobo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd27300",
   "metadata": {},
   "source": [
    "#### Portland State University, Electrical and Computer Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f86a71",
   "metadata": {},
   "source": [
    "Dependencies: keras-nightly==2.5.0.dev2021032900 PennyLane==0.17.0 StrawberryFields==0.18.0 tensorflow-2.4.0-cp38-cp38-macosx_10_9_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdfd7e7",
   "metadata": {},
   "source": [
    "# 4-qumode classifier\n",
    "\n",
    "Classical and Continuous Variable Quantum hybrid network: Classical layers using keras dense and quantum layers using Pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0767814",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 14:23:04.631953: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.layers import Reshape,Dense,Dropout,Flatten\n",
    "from tensorflow.keras.layers import Conv2D \n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a885b4",
   "metadata": {},
   "source": [
    "## 0. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79e9f8",
   "metadata": {},
   "source": [
    "Normalize pixel values from 0 ~ 255 to 0 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0614ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 2000\n",
      "[b'AM-SSB' b'QAM64' b'QPSK' b'WBFM']\n"
     ]
    }
   ],
   "source": [
    "Xd = pickle.load(open(\"./RML2016.10a_dict.pkl\",'rb'),encoding = \"bytes\")\n",
    "snrs,mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "\n",
    "# remove 7 modulations to only train on 4 modulations\n",
    "#mods.remove(b'QPSK')\n",
    "mods.remove(b'8PSK')\n",
    "mods.remove(b'AM-DSB')\n",
    "#mods.remove(b'AM-SSB')\n",
    "mods.remove(b'PAM4')\n",
    "mods.remove(b'QAM16')\n",
    "#mods.remove(b'QAM64')\n",
    "mods.remove(b'BPSK')\n",
    "mods.remove(b'CPFSK')\n",
    "mods.remove(b'GFSK')\n",
    "#mods.remove(b'WBFM')\n",
    "\n",
    "# read in data\n",
    "X = []\n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))\n",
    "X = np.vstack(X)\n",
    "\n",
    "# Partition the data into training and test sets \n",
    "np.random.seed(2023)\n",
    "n_examples = X.shape[0]\n",
    "n_train = (3 * n_examples) // 4        # 75% to train\n",
    "\n",
    "idx = np.random.choice(range(0,n_examples), size=n_examples, replace=False)\n",
    "train_idx = idx[0:n_train]\n",
    "test_idx = idx[n_train:n_examples]\n",
    "\n",
    "div_factor = 10    # To use 100th of the dataset because 200,000 vectors may be too computationally expensive to train\n",
    "\n",
    "train_idx = train_idx[0:len(train_idx)//div_factor]\n",
    "test_idx = test_idx[0:len(test_idx)//div_factor]\n",
    "print(len(train_idx), len(test_idx))\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test =  X[test_idx]\n",
    "\n",
    "#one-hot encode the labels\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(np.asarray(lbl)[:,0])\n",
    "print(lb.classes_)\n",
    "lbl_encoded=lb.transform(np.asarray(lbl)[:,0])\n",
    "y_train=lbl_encoded[train_idx]\n",
    "y_test=lbl_encoded[test_idx]\n",
    "\n",
    "in_shp = list(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea2418",
   "metadata": {},
   "source": [
    "One hot encode labels to vectors of size cutoff_dim^(num_qumodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdf7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):  \n",
    "       \n",
    "    depth =  2**4                       # 4 classes + 12 zeros for padding\n",
    "    indices = labels.astype(np.int32)    \n",
    "    one_hot_labels = np.eye(depth)[indices].astype(np.float32) \n",
    "    \n",
    "    return one_hot_labels\n",
    "\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# one-hot encoded labels, each label of length cutoff dimension**2\n",
    "y_train, y_test = one_hot(y_train), one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bdc53d",
   "metadata": {},
   "source": [
    "## 1. Classical circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530091b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 14:23:21.394154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 2, 128, 1)         0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 2, 113, 64)        1088      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 113, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 106, 32)        32800     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 106, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 103, 16)        2064      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 103, 16)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1648)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                49470     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,422\n",
      "Trainable params: 85,422\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "# Define classical layers using Keras Sequential. Take in 2x128 radio modulations, flatten, and output vectors of length 30. 2 hidden layers with ELU activation.\n",
    "model = models.Sequential()\n",
    "model.add(Reshape(in_shp + [1], input_shape = in_shp))\n",
    "\n",
    "model.add(Conv2D(64, (1, 16), activation ='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "\n",
    "model.add(Conv2D(32, (2, 8), activation ='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, (1, 4), activation ='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(30, activation ='sigmoid'))\n",
    "\n",
    "\n",
    "# More than a million parameters for the classical circuit\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d42ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6cab55",
   "metadata": {},
   "source": [
    "## 2. Data encoding circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b80f5",
   "metadata": {},
   "source": [
    "Encode the output vectors from the classical network into quantum states using the vector entries as parameters of continuous variable gates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a847d24a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encode_data(x):\n",
    "    qml.Squeezing(x[0], x[1], wires=0)\n",
    "    qml.Squeezing(x[2], x[3], wires=1)\n",
    "    qml.Squeezing(x[4], x[5], wires=2)\n",
    "    qml.Squeezing(x[6], x[7], wires=3)\n",
    "    \n",
    "    qml.Beamsplitter(x[8], x[9], wires=[0,1])\n",
    "    qml.Beamsplitter(x[10], x[11], wires=[1,2])\n",
    "    qml.Beamsplitter(x[12], x[13], wires=[2,3])\n",
    "    \n",
    "    qml.Rotation(x[14], wires=0)\n",
    "    qml.Rotation(x[15], wires=1)\n",
    "    qml.Rotation(x[16], wires=2)\n",
    "    qml.Rotation(x[17], wires=3)    \n",
    "    \n",
    "    qml.Displacement(x[18], x[19], wires=0)\n",
    "    qml.Displacement(x[20], x[21], wires=1)\n",
    "    qml.Displacement(x[22], x[23], wires=2)\n",
    "    qml.Displacement(x[24], x[25], wires=3) \n",
    "    \n",
    "    qml.Kerr(x[26], wires=0)\n",
    "    qml.Kerr(x[27], wires=1)\n",
    "    qml.Kerr(x[28], wires=2)\n",
    "    qml.Kerr(x[29], wires=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d911b",
   "metadata": {},
   "source": [
    "## 3. Qauntum neural network circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f959fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def layer(v):\n",
    "    \n",
    "    # Linear transformation W = Interferemeter, squeezers, interferometer\n",
    "    # Interferometer 1\n",
    "    qml.Beamsplitter(v[0], v[1], wires=[0,1])\n",
    "    qml.Beamsplitter(v[2], v[3], wires=[1,2])\n",
    "    qml.Beamsplitter(v[4], v[5], wires=[2,3])\n",
    "    \n",
    "    qml.Rotation(v[6], wires=0)\n",
    "    qml.Rotation(v[7], wires=1)\n",
    "    qml.Rotation(v[8], wires=2)\n",
    "    qml.Rotation(v[9], wires=3)\n",
    "    \n",
    "    # Squeezers\n",
    "    qml.Squeezing(v[10], v[11], wires=0)\n",
    "    qml.Squeezing(v[12], v[13], wires=1)\n",
    "    qml.Squeezing(v[14], v[15], wires=2)\n",
    "    qml.Squeezing(v[16], v[17], wires=3) \n",
    "    \n",
    "    # Interferometer 2\n",
    "    qml.Beamsplitter(v[18], v[19], wires=[0,1])\n",
    "    qml.Beamsplitter(v[20], v[21], wires=[1,2])\n",
    "    qml.Beamsplitter(v[22], v[23], wires=[2,3])\n",
    "    \n",
    "    qml.Rotation(v[24], wires=0)\n",
    "    qml.Rotation(v[25], wires=1)\n",
    "    qml.Rotation(v[26], wires=2)\n",
    "    qml.Rotation(v[27], wires=3)\n",
    "    \n",
    "    # Bias addition\n",
    "    qml.Displacement(v[28], v[29], wires=0)\n",
    "    qml.Displacement(v[30], v[31], wires=1)\n",
    "    qml.Displacement(v[32], v[33], wires=2)\n",
    "    qml.Displacement(v[34], v[35], wires=3)\n",
    "    \n",
    "    # Non-linear activation\n",
    "    qml.Kerr(v[36], wires=0)\n",
    "    qml.Kerr(v[37], wires=1)\n",
    "    qml.Kerr(v[38], wires=2)\n",
    "    qml.Kerr(v[39], wires=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eeeb3c",
   "metadata": {},
   "source": [
    "## 4. Quantum device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9adba",
   "metadata": {},
   "source": [
    "For the expression of qumodes in Fock basis, choose a \"strawberryfields.fock\" device. Define the number of qumodes and cutoff dimension. Run the data encoding circuit and quantum neural network circuit. The probability measurement method (qml.probs(wires)) returns vectors of size 2^4 = 16 (cutoff_dim^num_modes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3743469d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "num_modes = 4\n",
    "cutoff_dim = 2\n",
    "\n",
    "# select a devide \n",
    "dev = qml.device(\"strawberryfields.fock\", wires=num_modes, cutoff_dim=cutoff_dim) \n",
    "\n",
    "@qml.qnode(dev, interface=\"tf\")\n",
    "def quantum_nn(inputs, var):\n",
    "    # Encode input x into quantum state\n",
    "    encode_data(inputs)\n",
    "\n",
    "    # iterative quantum layers\n",
    "    for v in var:\n",
    "        layer(v)\n",
    "\n",
    "    # Encode input x into quantum state\n",
    "    encode_data(inputs)\n",
    "\n",
    "    # iterative quantum layers\n",
    "    for v in var:\n",
    "        layer(v)\n",
    "\n",
    "    return qml.probs(wires=[0, 1, 2, 3])  # Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f1652a",
   "metadata": {},
   "source": [
    "## 5. Hybrid circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "427bfa2d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "weight_shape = {'var': (num_layers, 40)}          # 4 layers and 40 parameters per layer, Keras layer will initialize.\n",
    "\n",
    "qlayer = qml.qnn.KerasLayer(quantum_nn, weight_shape, output_dim = 4)\n",
    "\n",
    "# add to the classical sequential model\n",
    "model.add(qlayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435522c",
   "metadata": {},
   "source": [
    "## 6. Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698c62d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#opt = keras.optimizers.SGD(lr = 0.02)\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.02)\n",
    "model.compile(opt, loss = 'categorical_crossentropy', metrics =['accuracy'])\n",
    "class ModelCheckpoint(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_accuracy = 0.0000  # Initial value is 0, if warm-starting use value of the best validation accuracy so far\n",
    "        self.f1 = open(\"loss_accuracy_CV_4_qumodes_co4\", 'a')   # change name for your specific case\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "#        print(self.model.get_layer('dense').get_weights())\n",
    "#        print(self.model.get_layer('keras_layer').get_weights())\n",
    "        self.f1.write(\"%f %f %f %f\\n\" % (logs['loss'], logs['accuracy'], logs['val_loss'], logs['val_accuracy']))\n",
    "        self.f1.flush()\n",
    "        if logs['val_accuracy'] > self.best_accuracy:\n",
    "            print(\"saving weights\")\n",
    "            self.best_accuracy = logs['val_accuracy']\n",
    "            self.model.save_weights('RML_CV_4_qumodes_co4')   # change name for your specific case\n",
    "\n",
    "ckpt = ModelCheckpoint()\n",
    "\n",
    "\n",
    "#Uncomment the following two lines for warm start\n",
    "#model.load_weights('RML_CV_4_qumodes_co2')\n",
    "#print('loaded model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41122ee0",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22bbaf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.5539 - accuracy: 0.2500   saving weights\n",
      "24/24 [==============================] - 22915s 961s/step - loss: 1.5539 - accuracy: 0.2500 - val_loss: 1.4015 - val_accuracy: 0.2350\n",
      "Epoch 2/150\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3955 - accuracy: 0.2563   saving weights\n",
      "24/24 [==============================] - 16534s 692s/step - loss: 1.3955 - accuracy: 0.2563 - val_loss: 1.3908 - val_accuracy: 0.2595\n",
      "Epoch 3/150\n",
      "24/24 [==============================] - 37529s 1595s/step - loss: 1.3899 - accuracy: 0.2457 - val_loss: 1.3891 - val_accuracy: 0.2595\n",
      "Epoch 4/150\n",
      "24/24 [==============================] - 29180s 1236s/step - loss: 1.3890 - accuracy: 0.2468 - val_loss: 1.3873 - val_accuracy: 0.2445\n",
      "Epoch 5/150\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3903 - accuracy: 0.2412   saving weights\n",
      "24/24 [==============================] - 15733s 654s/step - loss: 1.3903 - accuracy: 0.2412 - val_loss: 1.3909 - val_accuracy: 0.2610\n",
      "Epoch 6/150\n",
      "24/24 [==============================] - 14632s 610s/step - loss: 1.3903 - accuracy: 0.2540 - val_loss: 1.3948 - val_accuracy: 0.2595\n",
      "Epoch 7/150\n",
      "24/24 [==============================] - 27967s 1083s/step - loss: 1.3886 - accuracy: 0.2550 - val_loss: 1.3943 - val_accuracy: 0.2595\n",
      "Epoch 8/150\n",
      "13/24 [===============>..............] - ETA: 2:16:47 - loss: 1.3892 - accuracy: 0.2527"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hybrid \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, \n\u001b[1;32m      2\u001b[0m                    y_train,\n\u001b[1;32m      3\u001b[0m                    epochs \u001b[39m=\u001b[39;49m \u001b[39m150\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                    batch_size \u001b[39m=\u001b[39;49m \u001b[39m256\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m                    shuffle \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      6\u001b[0m                    validation_data \u001b[39m=\u001b[39;49m (X_test, y_test),\n\u001b[1;32m      7\u001b[0m                    callbacks \u001b[39m=\u001b[39;49m [ckpt])\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/engine/training.py:1160\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_function\u001b[39m(iterator):\n\u001b[1;32m   1159\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m     \u001b[39mreturn\u001b[39;00m step_function(\u001b[39mself\u001b[39;49m, iterator)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/engine/training.py:1146\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1143\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1145\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1146\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1147\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1148\u001b[0m     outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m )\n\u001b[1;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1315\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1311\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1314\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1315\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2891\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2889\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   2890\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2891\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3692\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3691\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 3692\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/engine/training.py:1135\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1135\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1136\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/engine/training.py:997\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m    996\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m--> 997\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m    998\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:576\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    546\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_gradients(\n\u001b[1;32m    577\u001b[0m         loss, var_list\u001b[39m=\u001b[39;49mvar_list, grad_loss\u001b[39m=\u001b[39;49mgrad_loss, tape\u001b[39m=\u001b[39;49mtape\n\u001b[1;32m    578\u001b[0m     )\n\u001b[1;32m    579\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:634\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    632\u001b[0m var_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(var_list)\n\u001b[1;32m    633\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/gradients\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_gradients(\n\u001b[1;32m    635\u001b[0m         tape, loss, var_list, grad_loss\n\u001b[1;32m    636\u001b[0m     )\n\u001b[1;32m    638\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assert_valid_dtypes(\n\u001b[1;32m    639\u001b[0m     [\n\u001b[1;32m    640\u001b[0m         v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     ]\n\u001b[1;32m    644\u001b[0m )\n\u001b[1;32m    646\u001b[0m \u001b[39mreturn\u001b[39;00m grads_and_vars\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:510\u001b[0m, in \u001b[0;36mOptimizerV2._get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_gradients\u001b[39m(\u001b[39mself\u001b[39m, tape, loss, var_list, grad_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    509\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 510\u001b[0m     grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list, grad_loss)\n\u001b[1;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py:1113\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1107\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1108\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1109\u001b[0m           output_gradients))\n\u001b[1;32m   1110\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1111\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1113\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1114\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1115\u001b[0m     flat_targets,\n\u001b[1;32m   1116\u001b[0m     flat_sources,\n\u001b[1;32m   1117\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1118\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1119\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1122\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/tensorflow/python/ops/custom_gradient.py:566\u001b[0m, in \u001b[0;36m_eager_mode_decorator.<locals>.actual_grad_fn\u001b[0;34m(*result_grads)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMust return gradient for each variable from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m@custom_gradient grad_fn.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m   input_grads \u001b[39m=\u001b[39m grad_fn(\u001b[39m*\u001b[39;49mresult_grads)\n\u001b[1;32m    567\u001b[0m   variable_grads \u001b[39m=\u001b[39m []\n\u001b[1;32m    568\u001b[0m flat_grads \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mflatten(input_grads)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/pennylane/interfaces/tensorflow.py:240\u001b[0m, in \u001b[0;36mexecute.<locals>._execute.<locals>.grad_fn\u001b[0;34m(*dy, **tfkwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[39mwith\u001b[39;00m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mUnwrap(\u001b[39m*\u001b[39mtapes, params\u001b[39m=\u001b[39mparams_unwrapped):\n\u001b[1;32m    232\u001b[0m         vjp_tapes, processing_fn \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mgradients\u001b[39m.\u001b[39mbatch_vjp(\n\u001b[1;32m    233\u001b[0m             tapes,\n\u001b[1;32m    234\u001b[0m             dy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m             gradient_kwargs\u001b[39m=\u001b[39mgradient_kwargs,\n\u001b[1;32m    238\u001b[0m         )\n\u001b[0;32m--> 240\u001b[0m         vjps \u001b[39m=\u001b[39m processing_fn(execute_fn(vjp_tapes)[\u001b[39m0\u001b[39m])\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     vjp_tapes, processing_fn \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mgradients\u001b[39m.\u001b[39mbatch_vjp(\n\u001b[1;32m    244\u001b[0m         tapes,\n\u001b[1;32m    245\u001b[0m         dy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m         gradient_kwargs\u001b[39m=\u001b[39mgradient_kwargs,\n\u001b[1;32m    249\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/pennylane/interfaces/execution.py:205\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[39mreturn\u001b[39;00m (res, []) \u001b[39mif\u001b[39;00m return_tuple \u001b[39melse\u001b[39;00m res\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[39m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     res \u001b[39m=\u001b[39m fn(execution_tapes\u001b[39m.\u001b[39;49mvalues(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m final_res \u001b[39m=\u001b[39m []\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m i, tape \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tapes):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/pennylane/interfaces/execution.py:131\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(tapes: Sequence[QuantumTape], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     tapes \u001b[39m=\u001b[39m [expand_fn(tape) \u001b[39mfor\u001b[39;00m tape \u001b[39min\u001b[39;00m tapes]\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn(tapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/pennylane/_device.py:530\u001b[0m, in \u001b[0;36mDevice.batch_execute\u001b[0;34m(self, circuits)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m circuit \u001b[39min\u001b[39;00m circuits:\n\u001b[1;32m    526\u001b[0m     \u001b[39m# we need to reset the device here, else it will\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[39m# not start the next computation in the zero state\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 530\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(circuit\u001b[39m.\u001b[39;49moperations, circuit\u001b[39m.\u001b[39;49mobservables)\n\u001b[1;32m    531\u001b[0m     results\u001b[39m.\u001b[39mappend(res)\n\u001b[1;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mactive:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/pennylane/_device.py:459\u001b[0m, in \u001b[0;36mDevice.execute\u001b[0;34m(self, queue, observables, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(operation\u001b[39m.\u001b[39mname, operation\u001b[39m.\u001b[39mwires, operation\u001b[39m.\u001b[39mparameters)\n\u001b[1;32m    457\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_apply()\n\u001b[0;32m--> 459\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_measure()\n\u001b[1;32m    461\u001b[0m \u001b[39mfor\u001b[39;00m obs \u001b[39min\u001b[39;00m observables:\n\u001b[1;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, Tensor):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/pennylane_sf/fock.py:131\u001b[0m, in \u001b[0;36mStrawberryFieldsFock.pre_measure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_measure\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meng \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mEngine(\u001b[39m\"\u001b[39m\u001b[39mfock\u001b[39m\u001b[39m\"\u001b[39m, backend_options\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcutoff_dim\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcutoff})\n\u001b[0;32m--> 131\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meng\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprog)\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mstate\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/engine.py:570\u001b[0m, in \u001b[0;36mLocalEngine.run\u001b[0;34m(self, program, args, compile_options, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[39mif\u001b[39;00m c\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mmeasurement_deps \u001b[39mand\u001b[39;00m eng_run_options[\u001b[39m\"\u001b[39m\u001b[39mshots\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    566\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFeed-forwarding of measurements cannot be used together with multiple shots.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m             )\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_run(\n\u001b[1;32m    571\u001b[0m     program_lst, args\u001b[39m=\u001b[39;49margs, compile_options\u001b[39m=\u001b[39;49mcompile_options, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49meng_run_options\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/engine.py:306\u001b[0m, in \u001b[0;36mBaseEngine._run\u001b[0;34m(self, program, args, compile_options, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m p\u001b[39m.\u001b[39mbind_params(args)\n\u001b[1;32m    304\u001b[0m p\u001b[39m.\u001b[39mlock()\n\u001b[0;32m--> 306\u001b[0m _, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_program(p, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_progs\u001b[39m.\u001b[39mappend(p)\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p, TDMProgram) \u001b[39mand\u001b[39;00m received_rolled:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/engine.py:430\u001b[0m, in \u001b[0;36mLocalEngine._run_program\u001b[0;34m(self, prog, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mfor\u001b[39;00m cmd \u001b[39min\u001b[39;00m prog\u001b[39m.\u001b[39mcircuit:\n\u001b[1;32m    428\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m         \u001b[39m# try to apply it to the backend and, if op is a measurement, store it in values\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m         val \u001b[39m=\u001b[39m cmd\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mapply(cmd\u001b[39m.\u001b[39;49mreg, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    431\u001b[0m         \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m             \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cmd\u001b[39m.\u001b[39mreg):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/ops.py:508\u001b[0m, in \u001b[0;36mGate.apply\u001b[0;34m(self, reg, backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m temp \u001b[39m=\u001b[39m [rr\u001b[39m.\u001b[39mind \u001b[39mfor\u001b[39;00m rr \u001b[39min\u001b[39;00m reg]\n\u001b[1;32m    507\u001b[0m \u001b[39m# call the child class specialized _apply method\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(temp, backend, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    509\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m original_p0\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/ops.py:1854\u001b[0m, in \u001b[0;36mRgate._apply\u001b[0;34m(self, reg, backend, **kwargs)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, reg, backend, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1853\u001b[0m     p \u001b[39m=\u001b[39m par_evaluate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp)\n\u001b[0;32m-> 1854\u001b[0m     backend\u001b[39m.\u001b[39;49mrotation(p[\u001b[39m0\u001b[39;49m], \u001b[39m*\u001b[39;49mreg)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/backends/fockbackend/backend.py:167\u001b[0m, in \u001b[0;36mFockBackend.rotation\u001b[0;34m(self, phi, mode)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrotation\u001b[39m(\u001b[39mself\u001b[39m, phi, mode):\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcircuit\u001b[39m.\u001b[39;49mphase_shift(phi, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_remap_modes(mode))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tamu_py38/lib/python3.8/site-packages/strawberryfields/backends/fockbackend/circuit.py:541\u001b[0m, in \u001b[0;36mCircuit.phase_shift\u001b[0;34m(self, theta, mode)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mphase_shift\u001b[39m(\u001b[39mself\u001b[39m, theta, mode):\n\u001b[1;32m    538\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m    Applies a phase shifter.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     mat \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mphase(theta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trunc)\n\u001b[1;32m    542\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gate_BLAS(mat, [mode])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hybrid = model.fit(X_train, \n",
    "                   y_train,\n",
    "                   epochs = 150,\n",
    "                   batch_size = 256,\n",
    "                   shuffle = True, \n",
    "                   validation_data = (X_test, y_test),\n",
    "                   callbacks = [ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3389d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7626b81",
   "metadata": {},
   "source": [
    "## 8. Loss and accuracy graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a1b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                  Loss History Plot\n",
    "# ===================================================================================\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.plot(hybrid.history['loss'], '-g')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae03bc",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "#                                Accuracy History Plot\n",
    "# ===================================================================================\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.plot(hybrid.history['accuracy'], '-g')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tamu_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
